# deep-learning-challenge
The objective is to develop a machine learning techniques, particularly neural networks, to predict whether applicants for funding from Alphabet Soup will be successful in their ventures. This classifier will analyze the provided dataset, which includes information on over 34,000 organizations that have received funding from Alphabet Soup. The dataset contains various features such as application type, affiliation, classification, use case, organization type, income amount, special considerations, funding amount requested, and whether the funding was used effectively.

## Data Source
Data for this dataset was generated by edX Boot Camps LLC. 
References
IRS. Tax Exempt Organization Search Bulk Data Downloads. https://www.irs.gov/Links to an external site.


## Instructions:
•    A new repository  called deep-learning challenge was created, cloned and pushed to GitHub.
The instructions for this Challenge are divided into the following subsections:
1.    Preprocess the Data
2.    Compile, Train, and Evaluate the Model
3.    Optimize the Model
4.    Write a Report on the Neural Network Model
5.    Copy Files Into Your Repository

Step 1: Preprocess the Data
Start by uploading the starter file to Google Colab, then follow the instructions to complete the preprocessing steps.
Read in the charity_data.csv to a Pandas DataFrame and find :
•    What variable(s) are the target(s) for your model?
•    What variable(s) are the feature(s) for your model?
•    Drop the EIN and NAME columns.
•    Determine the number of unique values for each column.
•    For columns that have more than 10 unique values, determine the number of data points for each unique value.
•    Use the number of data points for each unique value to pick a cutoff point to combine "rare" categorical variables together in a new value, Other, and then check if the replacement was successful.
•    Use pd.get_dummies() to encode categorical variables.
•    Split the preprocessed data into a features array, X, and a target array, y. Use these arrays and the train_test_split function to split the data into training and testing datasets.
•    Scale the training and testing features datasets by creating a StandardScaler instance, fitting it to the training data, then using the transform function.

Step 2: Compile, Train, and Evaluate the Model
•    Create a neural network model by assigning the number of input features and nodes for each layer using TensorFlow and Keras.
•    Create the first hidden layer and choose an appropriate activation function.If necessary, add a second hidden layer with an appropriate activation function.
•    Create an output layer with an appropriate activation function.
•    Check the structure of the model.
•    Compile and train the model.
•    Create a callback that saves the model's weights every five epochs.
•    Evaluate the model using the test data to determine the loss and accuracy.
•    Save and export your results to an HDF5 file. Name the file AlphabetSoupCharity.h5.

Step 3: Optimize the Model
•    Create a new Google Colab file and name it AlphabetSoupCharity_Optimization.ipynb.
•    Import your dependencies and read in the charity_data.csv to a Pandas DataFrame.Preprocess the dataset as you did in Step 1.
•    Design a neural network model, and adjust for modifications that will optimize the model to achieve higher than 75% accuracy.
•    Adjust the input data to ensure that no variables or outliers are causing confusion in the model, such as:
•    Dropping more or fewer columns.
•    Creating more bins for rare occurrences in columns.
•    Increasing or decreasing the number of values for each bin.
•    Add more neurons to a hidden layer.
•    Add more hidden layers.
•    Use different activation functions for the hidden layers.
•    Add or reduce the number of epochs to the training regimen.
•    Save and export your results to an HDF5 file. Name the file AlphabetSoupCharity_Optimization.h5.

Step 4: Write a Report on the Neural Network Model
The report consist of :
•    Overview of the analysis: Explain the purpose of this analysis.
•    Results: Using bulleted lists and images to support your answers, address the following questions:
-    Data Preprocessing
-    What variable(s) are the target(s) for your model?
-    What variable(s) are the features for your model?
-    What variable(s) should be removed from the input data because they are neither targets nor features?
-    Compiling, Training, and Evaluating the Model
-    How many neurons, layers, and activation functions did you select for your neural network model, and why?
-    Were you able to achieve the target model performance?
-    What steps did you take in your attempts to increase model performance?
•    Summary: Summarize the overall results of the deep learning model. Include a recommendation for how a different model could solve this classification problem, and then explain your recommendation.

Step 5: Copy Files Into Your Repository
•    Download your Colab notebooks and move them into your Deep Learning Challenge directory in your local repository.
•    Push the added files to GitHub.

### Prerequisites
-    Tools need to have installed before project:
•    Google Colab notebook
•    Pandas
•    Tenserflow
•    Sklearn
•    GitHub

## Analysis
•    A brief analysis report on deep neural network model is added at the end of AlphabetSoupCharity_Optimization.ipynb notebook.
